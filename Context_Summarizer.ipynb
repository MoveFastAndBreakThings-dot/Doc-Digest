{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K03ppundRcRD",
        "outputId": "fa343c8a-a9d3-451b-c269-6a8abb3adb9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.11)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers gradio pymupdf #used dependencies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Generative QA pipeline\n",
        "gen_qa = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=256)\n",
        "\n",
        "# New context: Transformers in NLP\n",
        "context = \"\"\"\n",
        "Transformers are a type of deep learning architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\n",
        "They have revolutionized natural language processing (NLP) by enabling models to process entire sequences of text at once using self-attention mechanisms, instead of relying on sequential processing like RNNs or LSTMs.\n",
        "\n",
        "A key innovation in transformers is the attention mechanism, which allows the model to weigh the relevance of different words in a sequence, regardless of their position. This results in better understanding of context and relationships in text.\n",
        "\n",
        "Transformers form the foundation of models such as BERT, GPT, T5, and RoBERTa, which are pre-trained on large corpora and then fine-tuned for specific tasks like sentiment analysis, text classification, translation, and question answering.\n",
        "\n",
        "Because of their scalability and performance, transformers have also been adopted in fields beyond NLP, such as computer vision, protein folding, and generative modeling.\n",
        "\"\"\"\n",
        "\n",
        "# New questions\n",
        "questions = [\n",
        "    \"What is a transformer in NLP?\",\n",
        "    \"Who introduced the transformer model and when?\",\n",
        "    \"What is the attention mechanism in transformers?\",\n",
        "    \"Which models are based on the transformer architecture?\",\n",
        "    \"Are transformers used outside of NLP?\"\n",
        "]\n",
        "\n",
        "# Ask each question\n",
        "for question in questions:\n",
        "    prompt = f\"Answer the question based on the context:\\nContext: {context}\\nQuestion: {question}\"\n",
        "    answer = gen_qa(prompt)[0]['generated_text']\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcrVoIAgfH8e",
        "outputId": "f14c683b-30e0-45b6-b7ed-7e27be3c9930"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is a transformer in NLP?\n",
            "A: enabling models to process entire sequences of text at once using self-attention mechanisms, instead of relying on sequential processing like RNNs or LSTMs\n",
            "\n",
            "Q: Who introduced the transformer model and when?\n",
            "A: Vaswani et al.\n",
            "\n",
            "Q: What is the attention mechanism in transformers?\n",
            "A: allows the model to weigh the relevance of different words in a sequence, regardless of their position\n",
            "\n",
            "Q: Which models are based on the transformer architecture?\n",
            "A: BERT, GPT, T5, and RoBERTa\n",
            "\n",
            "Q: Are transformers used outside of NLP?\n",
            "A: transformers have also been adopted in fields beyond NLP\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "from transformers import pipeline\n",
        "import math\n",
        "\n",
        "# Multilingual QA pipeline\n",
        "qa_models = {\n",
        "    \"English\": \"deepset/roberta-base-squad2\",\n",
        "    \"Multilingual\": \"deepset/xlm-roberta-large-squad2\",\n",
        "}\n",
        "\n",
        "# Clean text extracted from PDFs or TXT\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\n', ' ')  # Flatten line breaks\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
        "    return text.strip()\n",
        "\n",
        "# Extract text from PDF or TXT file\n",
        "def extract_text(file):\n",
        "    if file is None:\n",
        "        return \"\"\n",
        "    fname = file.name\n",
        "    if fname.endswith(\".pdf\"):\n",
        "        try:\n",
        "            doc = fitz.open(fname)\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            return clean_text(text)\n",
        "        except Exception as e:\n",
        "            return f\"‚ö†Ô∏è PDF extraction failed: {str(e)}\"\n",
        "    elif fname.endswith(\".txt\"):\n",
        "        try:\n",
        "            with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
        "                return clean_text(f.read())\n",
        "        except Exception as e:\n",
        "            return f\"‚ö†Ô∏è TXT read failed: {str(e)}\"\n",
        "    else:\n",
        "        return \"‚ö†Ô∏è Unsupported file type. Please upload PDF or TXT.\"\n",
        "\n",
        "# Simple tokenizer splitter by words to chunk text (approx 400 tokens)\n",
        "def chunk_text(text, chunk_size=400, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(words):\n",
        "        end = start + chunk_size\n",
        "        chunk = words[start:end]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        start += chunk_size - overlap  # overlap to maintain context continuity\n",
        "    return chunks\n",
        "\n",
        "# Load QA pipeline dynamically depending on language choice\n",
        "def get_qa_pipeline(language):\n",
        "    model_name = qa_models.get(language, \"deepset/roberta-base-squad2\")\n",
        "    return pipeline(\"question-answering\", model=model_name)\n",
        "\n",
        "# Answer question based on file(s) with chunking for long texts\n",
        "def answer_files_question(files, question, language):\n",
        "    if not files or not question.strip():\n",
        "        return \"‚ö†Ô∏è Please upload at least one file and enter a question.\"\n",
        "\n",
        "    combined_context = \"\"\n",
        "    for file in files:\n",
        "        text = extract_text(file)\n",
        "        if text.startswith(\"‚ö†Ô∏è\"):\n",
        "            return text\n",
        "        combined_context += text + \" \"\n",
        "\n",
        "    combined_context = combined_context.strip()\n",
        "    if not combined_context:\n",
        "        return \"‚ö†Ô∏è Uploaded files are empty or couldn't extract text.\"\n",
        "\n",
        "    try:\n",
        "        qa = get_qa_pipeline(language)\n",
        "        chunks = chunk_text(combined_context, chunk_size=400, overlap=50)\n",
        "\n",
        "        best_answer = None\n",
        "        best_score = -math.inf\n",
        "        for chunk in chunks:\n",
        "            result = qa(question=question, context=chunk)\n",
        "            if result['score'] > best_score:\n",
        "                best_score = result['score']\n",
        "                best_answer = result['answer']\n",
        "\n",
        "        answer = f\"### Answer:\\n{best_answer}\\n\\n*Confidence: {best_score:.2f}*\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "# Answer question based on pasted context text with chunking\n",
        "def answer_context_question(context, question, language):\n",
        "    if not context.strip() or not question.strip():\n",
        "        return \"‚ö†Ô∏è Please enter context and a question.\"\n",
        "    try:\n",
        "        cleaned_context = clean_text(context)\n",
        "        qa = get_qa_pipeline(language)\n",
        "        chunks = chunk_text(cleaned_context, chunk_size=400, overlap=50)\n",
        "\n",
        "        best_answer = None\n",
        "        best_score = -math.inf\n",
        "        for chunk in chunks:\n",
        "            result = qa(question=question, context=chunk)\n",
        "            if result['score'] > best_score:\n",
        "                best_score = result['score']\n",
        "                best_answer = result['answer']\n",
        "\n",
        "        answer = f\"### Answer:\\n{best_answer}\\n\\n*Confidence: {best_score:.2f}*\"\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"‚ö†Ô∏è Error: {str(e)}\"\n",
        "\n",
        "# Gradio UI (unchanged from your original)\n",
        "with gr.Blocks(theme=gr.themes.Default()) as demo:\n",
        "    gr.HTML(\"<script>document.title = 'DocDigest Multilingual';</script>\")\n",
        "    gr.Markdown(\"<h2 style='text-align:center; color:#4A90E2;'>üìö DocDigest - Multilingual QA & Multi-File Support</h2>\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### Upload one or more PDF/TXT files\")\n",
        "            file_input = gr.File(label=\"Upload PDF or TXT files\", file_types=['.pdf', '.txt'], file_count=\"multiple\")\n",
        "            file_question_input = gr.Textbox(label=\"Ask a question about the uploaded files\", placeholder=\"Type your question here...\", lines=2)\n",
        "            file_language = gr.Dropdown(choices=list(qa_models.keys()), value=\"English\", label=\"Select Language for QA Model\")\n",
        "            file_submit_btn = gr.Button(\"Get Answer for Files\")\n",
        "            file_answer_output = gr.Markdown()\n",
        "\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### Or paste context text here\")\n",
        "            context_input = gr.Textbox(label=\"Paste context text\", placeholder=\"Paste your text here...\", lines=10)\n",
        "            context_question_input = gr.Textbox(label=\"Ask a question about the context\", placeholder=\"Type your question here...\", lines=2)\n",
        "            context_language = gr.Dropdown(choices=list(qa_models.keys()), value=\"English\", label=\"Select Language for QA Model\")\n",
        "            context_submit_btn = gr.Button(\"Get Answer for Context\")\n",
        "            context_answer_output = gr.Markdown()\n",
        "\n",
        "    file_submit_btn.click(fn=answer_files_question, inputs=[file_input, file_question_input, file_language], outputs=file_answer_output)\n",
        "    context_submit_btn.click(fn=answer_context_question, inputs=[context_input, context_question_input, context_language], outputs=context_answer_output)\n",
        "\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "        body {\n",
        "            background-color: #f0f4f8;\n",
        "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
        "        }\n",
        "        #file_input input[type=\"file\"] {\n",
        "            width: 100% !important;\n",
        "            padding: 8px;\n",
        "            border-radius: 6px;\n",
        "            border: 1px solid #ccc;\n",
        "            box-sizing: border-box;\n",
        "        }\n",
        "        #file_question_input, #context_input, #context_question_input {\n",
        "            width: 100% !important;\n",
        "            font-size: 14px !important;\n",
        "            border-radius: 6px !important;\n",
        "            border: 1px solid #ccc !important;\n",
        "            padding: 8px !important;\n",
        "            box-sizing: border-box;\n",
        "        }\n",
        "        #file_submit_btn button, #context_submit_btn button {\n",
        "            background-color: #4A90E2 !important;\n",
        "            color: white !important;\n",
        "            font-weight: 600 !important;\n",
        "            font-size: 16px !important;\n",
        "            border-radius: 6px !important;\n",
        "            padding: 10px 20px !important;\n",
        "            margin-top: 10px !important;\n",
        "            width: 100% !important;\n",
        "            border: none !important;\n",
        "            cursor: pointer !important;\n",
        "            transition: background-color 0.3s ease;\n",
        "        }\n",
        "        #file_submit_btn button:hover, #context_submit_btn button:hover {\n",
        "            background-color: #357ABD !important;\n",
        "        }\n",
        "        #file_answer_output, #context_answer_output {\n",
        "            background-color: #fff !important;\n",
        "            padding: 15px !important;\n",
        "            border-radius: 6px !important;\n",
        "            min-height: 120px !important;\n",
        "            font-size: 15px !important;\n",
        "            color: #333 !important;\n",
        "            white-space: pre-wrap !important;\n",
        "            border: 1px solid #ddd !important;\n",
        "            margin-top: 10px !important;\n",
        "        }\n",
        "        </style>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ],
      "metadata": {
        "id": "cy299NPlh1ip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "d6237bcb-165a-4116-f9d9-b52a3d95908b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://28737dc382458dbdd1.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://28737dc382458dbdd1.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W5_rWsjEPvvN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}